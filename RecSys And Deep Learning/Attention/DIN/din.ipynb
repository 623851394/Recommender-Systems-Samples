{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        i, df = 0, {}\n",
    "        for line in f:\n",
    "            df[i] = eval(line)\n",
    "            i += 1\n",
    "        df = pd.DataFrame.from_dict(df, orient='index')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = to_df('./data/reviews_Electronics_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = to_df('./data/meta_Electronics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AO94DHGC771SJ</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>amazdnu</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>We got this GPS for my husband who is an (OTR)...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Gotta have GPS!</td>\n",
       "      <td>1370131200</td>\n",
       "      <td>06 2, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMO214LNFCEI4</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[12, 15]</td>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Very Disappointed</td>\n",
       "      <td>1290643200</td>\n",
       "      <td>11 25, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3N7T0DY83Y4IG</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>C. A. Freeman</td>\n",
       "      <td>[43, 45]</td>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1st impression</td>\n",
       "      <td>1283990400</td>\n",
       "      <td>09 9, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Dave M. Shaw \"mack dave\"</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Great grafics, POOR GPS</td>\n",
       "      <td>1290556800</td>\n",
       "      <td>11 24, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A24EV6RXELQZ63</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Wayne Smith</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Major issues, only excuses for support</td>\n",
       "      <td>1317254400</td>\n",
       "      <td>09 29, 2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin              reviewerName   helpful  \\\n",
       "0   AO94DHGC771SJ  0528881469                   amazdnu    [0, 0]   \n",
       "1   AMO214LNFCEI4  0528881469           Amazon Customer  [12, 15]   \n",
       "2  A3N7T0DY83Y4IG  0528881469             C. A. Freeman  [43, 45]   \n",
       "3  A1H8PY3QHMQQA0  0528881469  Dave M. Shaw \"mack dave\"   [9, 10]   \n",
       "4  A24EV6RXELQZ63  0528881469               Wayne Smith    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  We got this GPS for my husband who is an (OTR)...      5.0   \n",
       "1  I'm a professional OTR truck driver, and I bou...      1.0   \n",
       "2  Well, what can I say.  I've had this unit in m...      3.0   \n",
       "3  Not going to write a long review, even thought...      2.0   \n",
       "4  I've had mine for a year and here's what we go...      1.0   \n",
       "\n",
       "                                  summary  unixReviewTime   reviewTime  \n",
       "0                         Gotta have GPS!      1370131200   06 2, 2013  \n",
       "1                       Very Disappointed      1290643200  11 25, 2010  \n",
       "2                          1st impression      1283990400   09 9, 2010  \n",
       "3                 Great grafics, POOR GPS      1290556800  11 24, 2010  \n",
       "4  Major issues, only excuses for support      1317254400  09 29, 2011  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>imUrl</th>\n",
       "      <th>description</th>\n",
       "      <th>categories</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>related</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0132793040</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/31JIPhp%...</td>\n",
       "      <td>The Kelby Training DVD Mastering Blend Modes i...</td>\n",
       "      <td>[[Electronics, Computers &amp; Accessories, Cables...</td>\n",
       "      <td>Kelby Training DVD: Mastering Blend Modes in A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0321732944</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/31uogm6Y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Electronics, Computers &amp; Accessories, Cables...</td>\n",
       "      <td>Kelby Training DVD: Adobe Photoshop CS5 Crash ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0439886341</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51k0qa8f...</td>\n",
       "      <td>Digital Organizer and Messenger</td>\n",
       "      <td>[[Electronics, Computers &amp; Accessories, PDAs, ...</td>\n",
       "      <td>Digital Organizer and Messenger</td>\n",
       "      <td>8.15</td>\n",
       "      <td>{'Electronics': 144944}</td>\n",
       "      <td>{'also_viewed': ['0545016266', 'B009ECM8QY', '...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0511189877</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41HaAhbv...</td>\n",
       "      <td>The CLIKR-5 UR5U-8780L remote control is desig...</td>\n",
       "      <td>[[Electronics, Accessories &amp; Supplies, Audio &amp;...</td>\n",
       "      <td>CLIKR-5 Time Warner Cable Remote Control UR5U-...</td>\n",
       "      <td>23.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'also_viewed': ['B001KC08A4', 'B00KUL8O0W', '...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0528881469</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51FnRkJq...</td>\n",
       "      <td>Like its award-winning predecessor, the Intell...</td>\n",
       "      <td>[[Electronics, GPS &amp; Navigation, Vehicle GPS, ...</td>\n",
       "      <td>Rand McNally 528881469 7-inch Intelliroute TND...</td>\n",
       "      <td>299.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'also_viewed': ['B006ZOI9OY', 'B00C7FKT2A', '...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                              imUrl  \\\n",
       "0  0132793040  http://ecx.images-amazon.com/images/I/31JIPhp%...   \n",
       "1  0321732944  http://ecx.images-amazon.com/images/I/31uogm6Y...   \n",
       "2  0439886341  http://ecx.images-amazon.com/images/I/51k0qa8f...   \n",
       "3  0511189877  http://ecx.images-amazon.com/images/I/41HaAhbv...   \n",
       "4  0528881469  http://ecx.images-amazon.com/images/I/51FnRkJq...   \n",
       "\n",
       "                                         description  \\\n",
       "0  The Kelby Training DVD Mastering Blend Modes i...   \n",
       "1                                                NaN   \n",
       "2                    Digital Organizer and Messenger   \n",
       "3  The CLIKR-5 UR5U-8780L remote control is desig...   \n",
       "4  Like its award-winning predecessor, the Intell...   \n",
       "\n",
       "                                          categories  \\\n",
       "0  [[Electronics, Computers & Accessories, Cables...   \n",
       "1  [[Electronics, Computers & Accessories, Cables...   \n",
       "2  [[Electronics, Computers & Accessories, PDAs, ...   \n",
       "3  [[Electronics, Accessories & Supplies, Audio &...   \n",
       "4  [[Electronics, GPS & Navigation, Vehicle GPS, ...   \n",
       "\n",
       "                                               title   price  \\\n",
       "0  Kelby Training DVD: Mastering Blend Modes in A...     NaN   \n",
       "1  Kelby Training DVD: Adobe Photoshop CS5 Crash ...     NaN   \n",
       "2                    Digital Organizer and Messenger    8.15   \n",
       "3  CLIKR-5 Time Warner Cable Remote Control UR5U-...   23.36   \n",
       "4  Rand McNally 528881469 7-inch Intelliroute TND...  299.99   \n",
       "\n",
       "                 salesRank                                            related  \\\n",
       "0                      NaN                                                NaN   \n",
       "1                      NaN                                                NaN   \n",
       "2  {'Electronics': 144944}  {'also_viewed': ['0545016266', 'B009ECM8QY', '...   \n",
       "3                      NaN  {'also_viewed': ['B001KC08A4', 'B00KUL8O0W', '...   \n",
       "4                      NaN  {'also_viewed': ['B006ZOI9OY', 'B00C7FKT2A', '...   \n",
       "\n",
       "  brand  \n",
       "0   NaN  \n",
       "1   NaN  \n",
       "2   NaN  \n",
       "3   NaN  \n",
       "4   NaN  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24187\n",
      "20603\n"
     ]
    }
   ],
   "source": [
    "print(len(review_df['asin']))\n",
    "print(len(meta_df['asin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df[meta_df['asin'].isin(review_df['asin'].unique())]\n",
    "meta_df_reset = meta_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df[[\"reviewerID\",\"asin\",\"unixReviewTime\"]]\n",
    "meta_df = meta_df[[\"asin\", \"categories\"]]\n",
    "meta_df['categories'] = meta_df['categories'].map(lambda x:x[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin  unixReviewTime\n",
      "0   AO94DHGC771SJ  0528881469      1370131200\n",
      "1   AMO214LNFCEI4  0528881469      1290643200\n",
      "2  A3N7T0DY83Y4IG  0528881469      1283990400\n",
      "3  A1H8PY3QHMQQA0  0528881469      1290556800\n",
      "4  A24EV6RXELQZ63  0528881469      1317254400\n",
      "          asin                   categories\n",
      "4   0528881469                 Trucking GPS\n",
      "15  0594451647          Chargers & Adapters\n",
      "20  0594481813               Power Adapters\n",
      "38  0972683275     TV Ceiling & Wall Mounts\n",
      "53  1400532620  eBook Readers & Accessories\n",
      "24187\n",
      "971\n"
     ]
    }
   ],
   "source": [
    "print(review_df.head())\n",
    "print(meta_df.head())\n",
    "print(len(review_df['asin']))\n",
    "print(len(meta_df['asin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_map(df, col):\n",
    "    key = sorted(df[col].unique().tolist())\n",
    "    m = dict(zip(key, range(len(key))))\n",
    "    df[col] = df[col].map(lambda x:m[x])\n",
    "    return m, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_map, asin_key = build_map(meta_df, 'asin')\n",
    "cate_map, cate_key = build_map(meta_df, 'categories')\n",
    "rev_map, rev_key = build_map(review_df, 'reviewerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_count: 19168\titem_count: 971\tcate_count: 241\texample_count: 24187\n"
     ]
    }
   ],
   "source": [
    "user_count, item_count, cate_count, example_count = len(rev_map), len(asin_map), len(cate_map), review_df.shape[0]\n",
    "print('user_count: %d\\titem_count: %d\\tcate_count: %d\\texample_count: %d' %(user_count, item_count, cate_count, example_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   reviewerID        asin  unixReviewTime\n",
      "0       17558  0528881469      1370131200\n",
      "1       17340  0528881469      1290643200\n",
      "2       13395  0528881469      1283990400\n",
      "3        2433  0528881469      1290556800\n",
      "4        5745  0528881469      1317254400\n",
      "    asin  categories\n",
      "4      0         217\n",
      "15     1          47\n",
      "20     2         167\n",
      "38     3         206\n",
      "53     7         240\n",
      "24187\n",
      "971\n"
     ]
    }
   ],
   "source": [
    "print(review_df.head())\n",
    "print(meta_df.head())\n",
    "print(len(review_df['asin']))\n",
    "print(len(meta_df['asin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0528881469', '0528881469', '0528881469', '0528881469', '0528881469', '0594451647', '0594451647', '0594451647', '0594451647', '0594451647', '0594481813', '0594481813', '0594481813', '0594481813', '0594481813', '0594481813', '0594481813', '0594481813', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275', '0972683275']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(review_df['asin'])[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    asin  categories\n",
      "4      0         217\n",
      "15     1          47\n",
      "20     2         167\n",
      "38     3         206\n",
      "53     7         240\n",
      "   asin  categories\n",
      "0     0         217\n",
      "1     1          47\n",
      "2     2         167\n",
      "3     3         206\n",
      "4     4         210\n"
     ]
    }
   ],
   "source": [
    "print(meta_df.head())\n",
    "meta_df = meta_df.sort_values('asin').reset_index(drop=True)\n",
    "print(meta_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   reviewerID        asin  unixReviewTime\n",
      "0       17558  0528881469      1370131200\n",
      "1       17340  0528881469      1290643200\n",
      "2       13395  0528881469      1283990400\n",
      "3        2433  0528881469      1290556800\n",
      "4        5745  0528881469      1317254400\n",
      "   reviewerID  asin  unixReviewTime\n",
      "0           0    75      1385337600\n",
      "1           1   890      1358035200\n",
      "2           2   643      1361750400\n",
      "3           3   168      1390003200\n",
      "4           4   533      1350086400\n"
     ]
    }
   ],
   "source": [
    "print(review_df.head())\n",
    "review_df['asin'] = review_df['asin'].map(lambda x:asin_map[x])\n",
    "review_df1 = review_df.sort_values([\"reviewerID\",\"unixReviewTime\"]).reset_index(drop=True)\n",
    "review_df2 = review_df1[[\"reviewerID\",\"asin\",\"unixReviewTime\"]]\n",
    "print(review_df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   reviewerID  asin  unixReviewTime\n",
      "0           0    75      1385337600\n",
      "1           1   890      1358035200\n",
      "2           2   643      1361750400\n",
      "3           3   168      1390003200\n",
      "4           4   533      1350086400\n",
      "   asin  categories\n",
      "0     0         217\n",
      "1     1          47\n",
      "2     2         167\n",
      "3     3         206\n",
      "4     4         210\n"
     ]
    }
   ],
   "source": [
    "print(review_df2.head())\n",
    "print(meta_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "0      75\n",
      "1     890\n",
      "2     643\n",
      "3     168\n",
      "4     533\n",
      "5     303\n",
      "6     223\n",
      "7     346\n",
      "8     455\n",
      "9     860\n",
      "10    865\n",
      "11    666\n",
      "12    257\n",
      "13    179\n",
      "14    220\n",
      "15    223\n",
      "16      8\n",
      "17    123\n",
      "18    762\n",
      "19      3\n",
      "Name: asin, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sorted(review_df2['asin'])[:100])\n",
    "print(review_df2['asin'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = review_df2.groupby('reviewerID')\n",
    "tmp = sorted(tmp['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0    75\n",
      "Name: asin, dtype: int64), (1, 1    890\n",
      "Name: asin, dtype: int64), (2, 2    643\n",
      "Name: asin, dtype: int64), (3, 3    168\n",
      "Name: asin, dtype: int64), (4, 4    533\n",
      "Name: asin, dtype: int64), (5, 5    303\n",
      "Name: asin, dtype: int64), (6, 6    223\n",
      "7    346\n",
      "Name: asin, dtype: int64), (7, 8    455\n",
      "Name: asin, dtype: int64), (8, 9    860\n",
      "Name: asin, dtype: int64), (9, 10    865\n",
      "11    666\n",
      "12    257\n",
      "Name: asin, dtype: int64)]\n"
     ]
    }
   ],
   "source": [
    "print(tmp[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_list = np.array([meta_df['categories'][i] for i in range(len(asin_map))],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = [],[]\n",
    "for reviewerID, hist in review_df2.groupby('reviewerID'):\n",
    "    post_list = hist['asin'].tolist()\n",
    "\n",
    "    def gen_neg():\n",
    "        neg = post_list[0]\n",
    "        while(neg in post_list):\n",
    "            neg = random.randint(0, item_count-1)\n",
    "        return neg\n",
    "    neg_list = [gen_neg() for i in range(len(post_list))]\n",
    "    for i in range(1, len(post_list)):\n",
    "        hist = post_list[:i]\n",
    "        \n",
    "        if(i == len(post_list)-1):\n",
    "            test_set.append((reviewerID, hist, post_list[i], 1))\n",
    "        else:\n",
    "            train_set.append((reviewerID, hist, post_list[i], 1))\n",
    "            train_set.append((reviewerID, hist, neg_list[i], 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataInput(object):\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "#         self.epoch_size = (len(self.data)+self.batch_size-1) // self.batch_size\n",
    "        self.epoch_size = len(self.data) // self.batch_size\n",
    "        # 计算迭代次数\n",
    "        self.epoch_size + 1 if self.epoch_size * self.batch_size <len(self.data) else self.epoch_size\n",
    "        \n",
    "        self.idx = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if(self.idx == self.epoch_size):\n",
    "            raise StopIteration\n",
    "        start, end = self.idx*self.batch_size, min((self.idx+1)*self.batch_size, len(self.data))\n",
    "        b_data = self.data[start:end]\n",
    "        self.idx += 1\n",
    "        user_id, item_id, y, sample_len = [], [], [], []\n",
    "        for i in b_data:\n",
    "            user_id.append(i[0])\n",
    "            item_id.append(i[2])\n",
    "            y.append(i[3])\n",
    "            sample_len.append(len(i[1]))\n",
    "        max_sl = max(sample_len)\n",
    "        hist_matrix = np.zeros([len(b_data), max_sl], dtype=np.int64)\n",
    "        k = 0\n",
    "        for sample in b_data:\n",
    "            for j in range(len(sample[1])):\n",
    "                hist_matrix[k][j] = sample[1][j]\n",
    "            k += 1\n",
    "        return self.idx, (user_id, item_id, y, hist_matrix, sample_len)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "train_batch_size =32\n",
    "test_batch_size = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(s, axis=-1, eps=0.001, name=''):\n",
    "    '''\n",
    "    PRelu损失函数\n",
    "    B:batch\n",
    "    '''\n",
    "    with tf.variable_scope(name_or_scope='', reuse=tf.AUTO_REUSE):\n",
    "        alpha = tf.get_variable('alpha'+name, s.get_shape()[-1],                                  \n",
    "                             initializer=tf.constant_initializer(0.0),                         \n",
    "                             dtype=tf.float32)\n",
    "    \n",
    "    input_shape = list(s.get_shape()) # [1, hidden_size]\n",
    "    reduction_axis = list(range(len(input_shape))) # [0,1]\n",
    "    \n",
    "    del reduction_axis[axis]\n",
    "    \n",
    "#     broadcast_shape = [1] * len(input_shape) # [1, 1]\n",
    "#     broadcast_shape[axis] = input_shape[axis] # [1, hidden_size]\n",
    "#     mean = tf.reduce_mean(s, axis=reduction_axis) # [hidden_size,]\n",
    "#     broadcast_mean = tf.reshape(mean, broadcast_shape) # [1, hidden_size]\n",
    "#     std = tf.reduce_mean(tf.square(s-broadcast_mean), axis=reduction_axis) # [hidden_size,]\n",
    "#     std = tf.sqrt(std) # [hidden_size]\n",
    "#     broadcast_std = tf.reshape(std, broadcast_shape) # [1, hiddensize]\n",
    "#     s_normed = (s-broadcast_mean) / (broadcast_std+eps) # [B, hidden_size]\n",
    "\n",
    "    s_normed = tf.layers.batch_normalization(s, epsilon=eps, center=False,scale=False)\n",
    "    \n",
    "    p = tf.sigmoid(s_normed) # [B, hidden_size]\n",
    "    \n",
    "    return alpha*(1.0-p)*s+p*s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(queries, keys, keys_length):\n",
    "    '''\n",
    "    queries:     [B, H] item_emb\n",
    "    keys:        [B, T, H] hist_emb\n",
    "    keys_length: [B] sample_len\n",
    "    B:32,batch\n",
    "    T:用户感兴趣的物品个数的最大值\n",
    "    H:128,hidden_size\n",
    "    输入item_emb和用户的hist_emb，输出用户的特征表达\n",
    "    用户的特征表达用想关的历史物品hist_emb求和表示，此处加了attention赋予不同hist_emb不同权重\n",
    "    其中权重是和预测的物品相关的函数，预测物品不同，权重分布就不同，权重函数使用DNN实现\n",
    "    '''\n",
    "    queries_hidden_units = queries.get_shape().as_list()[-1]\n",
    "    # shape:0, data:H\n",
    "    queries = tf.tile(queries, [1, tf.shape(keys)[1]])\n",
    "    # shape:[B, T*H], data: (item, W)\n",
    "    queries = tf.reshape(queries, [-1, tf.shape(keys)[1], queries_hidden_units])\n",
    "    # shape:[B, T, H] data: (item, W)\n",
    "    din_all = tf.concat([queries, keys, queries-keys, queries*keys], axis=-1)\n",
    "    # shape:[B, T, 128*4]\n",
    "    d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att', reuse=tf.AUTO_REUSE)\n",
    "    d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att', reuse=tf.AUTO_REUSE)\n",
    "    d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=tf.nn.sigmoid, name='f3_att', reuse=tf.AUTO_REUSE)\n",
    "    d_layer_3_all = tf.reshape(d_layer_3_all, [-1, 1, tf.shape(keys)[1]])\n",
    "    # shape:[B, 1, T]\n",
    "    outputs = d_layer_3_all\n",
    "    # shape:[B, 1, T]\n",
    "    paddings = tf.ones_like(outputs)\n",
    "    # shape:[B, 1, T]\n",
    "    key_masks = tf.sequence_mask(keys_length, tf.shape(keys)[1])\n",
    "    # shape:[B T]\n",
    "    key_masks = tf.expand_dims(key_masks, 1)\n",
    "    # shape:[B,1,T]\n",
    "    outputs = tf.where(key_masks, outputs, paddings)\n",
    "    outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5)\n",
    "    # (B,1,T) / sqrt(H)， 进行标准化\n",
    "    outputs = tf.sigmoid(outputs)\n",
    "    # shape:[B, 1, T]\n",
    "    outputs = tf.matmul(outputs, keys)\n",
    "    # (B,1,T) * (B,T,H) ==> (B,1,H)\n",
    "    # 三维矩阵想乘，相乘发生在后两维\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, user_count, item_count, cate_count, cate_list):\n",
    "        # initialization of variables\n",
    "        self.user = tf.placeholder(tf.int32, [None, ]) # user id, 用户id列表\n",
    "        self.item = tf.placeholder(tf.int32, [None, ]) # id of items to predict，要预测的物品id列表\n",
    "        self.item_ = tf.placeholder(tf.int32, [None, ]) # item not like id，不感兴趣的物品id\n",
    "        self.sl = tf.placeholder(tf.int32, [None, ]) # sample len list，各用户兴趣个数列表\n",
    "        self.y = tf.placeholder(tf.float32, [None, ]) # label,0/1标签\n",
    "        self.hist = tf.placeholder(tf.int32, [None, None]) # [Batch, like]，用户兴趣二维列表\n",
    "        self.lr = tf.placeholder(tf.float64, []) # learning rate，学习率\n",
    "        hidden_units = 128\n",
    "        \n",
    "        with tf.variable_scope('weight', reuse=tf.AUTO_REUSE):\n",
    "            item_emb_w = tf.get_variable('item_emb_w', [item_count, hidden_units // 2])\n",
    "            item_emb_b = tf.get_variable('item_emb_b', [item_count], initializer=tf.constant_initializer(0.0))\n",
    "            cate_emb_w = tf.get_variable('cate_emb_w', [cate_count, hidden_units // 2])\n",
    "        cate_list = tf.convert_to_tensor(cate_list, dtype=tf.int64)\n",
    "        ##################################################################\n",
    "        # get item embedding vectors for input\n",
    "        \n",
    "        i_c = tf.gather(cate_list, self.item)  # obtain category of every item\n",
    "        item_emb = tf.concat(values=[\n",
    "            tf.nn.embedding_lookup(item_emb_w, self.item), # [B, H/2]\n",
    "            tf.nn.embedding_lookup(cate_emb_w, i_c)        # [B, H/2]\n",
    "        ], axis=1) # [B, H]\n",
    "        ##################################################################\n",
    "        # get hist embedding vectors for input\n",
    "        h_c = tf.gather(cate_list, self.hist)\n",
    "        hist_emb = tf.concat(values=[\n",
    "            tf.nn.embedding_lookup(item_emb_w, self.hist), # [B, T, H/2]\n",
    "            tf.nn.embedding_lookup(cate_emb_w, h_c)        # [B, T, H/2]\n",
    "        ], axis=2) # [B, T, H]\n",
    "        ##################################################################\n",
    "        # get user embedding vectors based on user hist vectors and item(to predict) vectors\n",
    "        att_hist_emb = attention(item_emb, hist_emb, self.sl)       # [B, 1, H]\n",
    "        att_hist_emb = tf.layers.batch_normalization(inputs=att_hist_emb)  # [B, 1, H]\n",
    "        att_hist_emb = tf.reshape(att_hist_emb, [-1, hidden_units]) # [B, H]\n",
    "        att_hist_emb = tf.layers.dense(att_hist_emb, hidden_units)  # [B, H]\n",
    "        user_emb = att_hist_emb\n",
    "        ##################################################################\n",
    "        # MLP\n",
    "        base_i = tf.concat([user_emb, item_emb], axis=-1) # [B, 2*H]\n",
    "        base_i = tf.layers.batch_normalization(base_i, name='base_i', reuse=tf.AUTO_REUSE) # [B, 2*H]\n",
    "        d_layer_1_i = tf.layers.dense(base_i, 80, activation=tf.nn.sigmoid, name='f1', reuse=tf.AUTO_REUSE) # [B, 80]\n",
    "        d_layer_1_i = dice(d_layer_1_i, name='dice1')\n",
    "        d_layer_2_i = tf.layers.dense(d_layer_1_i, 40, activation=tf.nn.sigmoid, name='f2', reuse=tf.AUTO_REUSE) # [B, 40]\n",
    "        d_layer_2_i = dice(d_layer_2_i, name='dice2')\n",
    "        d_layer_3_i = tf.layers.dense(d_layer_2_i, 1, activation=None, name='f3', reuse=tf.AUTO_REUSE) # [B, 1]\n",
    "        # 特征平铺\n",
    "        d_layer_3_i = tf.reshape(d_layer_3_i, [-1]) # [B,]\n",
    "        i_b = tf.gather(item_emb_b, self.item) # obtain bias of every item, [B,]\n",
    "        self.pre = i_b+d_layer_3_i # [B]\n",
    "        # global step\n",
    "        self.global_step = tf.Variable(0, trainable=False, name = 'global_step')\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=self.pre, \n",
    "                labels=self.y)\n",
    "        )\n",
    "        \n",
    "        trainable_params = tf.trainable_variables()\n",
    "        self.opt = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
    "        gradients = tf.gradients(self.loss, trainable_params)\n",
    "        clip_gradients, _ = tf.clip_by_global_norm(gradients, 5)\n",
    "        self.train_op = self.opt.apply_gradients(zip(clip_gradients, trainable_params),\n",
    "                                                global_step=self.global_step)\n",
    "    def train(self, sess, item, lr):\n",
    "        loss, _ =  sess.run([self.loss, self.train_op], feed_dict={\n",
    "            self.user:item[0],\n",
    "            self.item:item[1],\n",
    "            self.y:item[2],\n",
    "            self.hist:item[3],\n",
    "            self.sl:item[4],\n",
    "            self.lr:lr,\n",
    "        })\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 86.0931\n",
      "epoch: 1, loss: 85.8102\n"
     ]
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    model = Model(user_count, item_count, cate_count, cate_list)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    lr = 0.1\n",
    "    start_time = time.time()\n",
    "    for epoch in range(2):\n",
    "        random.shuffle(train_set)\n",
    "        loss_sum = 0.0\n",
    "        for idx, item in DataInput(train_set, train_batch_size):\n",
    "            loss = model.train(sess, item, lr)\n",
    "            loss_sum += loss\n",
    "            if(model.global_step.eval() % 336000 == 0):\n",
    "                lr  =0.0\n",
    "        print('epoch: %d, loss: %.4f' % (epoch, loss_sum))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
